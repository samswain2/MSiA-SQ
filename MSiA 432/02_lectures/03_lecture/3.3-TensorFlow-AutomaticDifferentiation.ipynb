{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Automatic Differentiation \n",
    "\n",
    "TensorFlow provides functions to compute the derivatives for a given TensorFlow computation graph, adding operations to the graph. The optimizer classes automatically compute derivatives on your graph, but creators of new Optimizers or expert users can call the lower-level functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Computation\n",
    "\n",
    "In order to compute gradient of function with respect to a variable you have to define both. Also you have to specify value at which you want to compute the gradient. \n",
    "\n",
    "<code>GradientTape</code> records operations for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5.0)\n",
    "\n",
    "#compute gradient of y=x**2\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x * x\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([[5.0, 10, 15.23232]])\n",
    "\n",
    "#compute gradient of y=x**2\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x * x * x\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "#compute gradient of y=x**2+x+1 with respect to x at 3\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2 + x - 1\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "print(grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "TensorFlow uses reverse mode automatic differentiation for it's gradients operation and finite difference method for tests that check validity of gradient operation. [Reverse mode automatic differentiation](https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation)  uses an extension of the forward mode computational graph to enable the computation of a gradient by a reverse traversal of the graph.\n",
    "\n",
    "Optimize the following:  $min (x + 1)^2$\n",
    "\n",
    "$\\frac{d}{dx} (x+1)^2 = 2*(x+1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.training import gradient_descent\n",
    "\n",
    "x = tf.Variable(3.0, trainable=True)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "@tf.function\n",
    "def f_x():\n",
    "    return x**2 + x - 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch :\", epoch, [x.numpy(), f_x().numpy()])\n",
    "    opt = gradient_descent.GradientDescentOptimizer(0.01).minimize(f_x)\n",
    "    tf.summary.scalar('loss', f_x().numpy(), step=epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
