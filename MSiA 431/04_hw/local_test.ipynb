{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o55.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mappName(\u001b[39m'\u001b[39m\u001b[39massignment\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mgetOrCreate()\n\u001b[1;32m     26\u001b[0m \u001b[39m# Load the data\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mcsv(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms3://\u001b[39;49m\u001b[39m{\u001b[39;49;00mdata_bucket\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mdata_key\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     29\u001b[0m \u001b[39m# Ensure the data types\u001b[39;00m\n\u001b[1;32m     30\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mtrade_id\u001b[39m\u001b[39m'\u001b[39m, col(\u001b[39m'\u001b[39m\u001b[39mtrade_id\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcast(IntegerType()))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py:727\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    726\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 727\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[1;32m    728\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    730\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o55.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, row_number, desc, to_timestamp, date_format\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import boto3\n",
    "\n",
    "### AWS S3 Setup ###\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "data_bucket = 'msia-432-hw4-data'\n",
    "data_key = 'full_data.csv'\n",
    "\n",
    "\n",
    "result_bucket = 'sms5736'\n",
    "result_prefix = '04_hw_431/'\n",
    "result_key = 'Exercise3.txt'\n",
    "result_location = result_prefix + result_key\n",
    "\n",
    "# Start a Spark Session\n",
    "spark = SparkSession.builder.appName('assignment').getOrCreate()\n",
    "\n",
    "# Load the data\n",
    "df = spark.read.csv(f's3://{data_bucket}/{data_key}', header=True)\n",
    "\n",
    "# Ensure the data types\n",
    "df = df.withColumn('trade_id', col('trade_id').cast(IntegerType()))\n",
    "df = df.withColumn('bar_num', col('bar_num').cast(IntegerType()))\n",
    "df = df.withColumn('profit', col('profit').cast(FloatType()))\n",
    "\n",
    "# Drop Rank\n",
    "df = df.drop('rank')\n",
    "\n",
    "for i in range(12, 79):  # feature columns\n",
    "    if f\"var{i}\" in df.columns:\n",
    "        df = df.withColumn(f'var{i}', col(f'var{i}').cast(FloatType()))\n",
    "\n",
    "# Add a column with the bar number relative to the nearest \"milestone\"\n",
    "df = df.withColumn(\"milestone\", ((col(\"bar_num\") - 1) / 10).cast(IntegerType()) * 10)\n",
    "\n",
    "# Calculate the average profit at each milestone\n",
    "milestone_df = df.filter(col('bar_num') % 10 == 0) \\\n",
    "    .groupBy('trade_id', 'milestone') \\\n",
    "    .agg(F.avg('profit').alias('avg_profit'))\n",
    "\n",
    "# Increment the milestone by 10 in the milestone_df (since this average will be used for the next milestone)\n",
    "milestone_df = milestone_df.withColumn('milestone', col('milestone') + 10)\n",
    "\n",
    "# Join the milestone averages back into the original DataFrame\n",
    "df = df.join(milestone_df, on=['trade_id', 'milestone'], how='left')\n",
    "\n",
    "# Forward fill the average profit values for each trade\n",
    "window_ffill = Window.partitionBy('trade_id').orderBy('bar_num')\n",
    "df = df.withColumn('avg_profit', F.last('avg_profit', ignorenulls=True).over(window_ffill))\n",
    "\n",
    "# Sort the data by trade_id and bar_num\n",
    "window = Window.orderBy(desc('trade_id'), col('bar_num'))\n",
    "df = df.withColumn('rank', row_number().over(window))\n",
    "\n",
    "window_lag = Window.partitionBy('trade_id').orderBy('bar_num')\n",
    "\n",
    "# # Get lag profits\n",
    "# for i in range(11, 20):  # lag profits for last 10 bars\n",
    "#     df = df.withColumn(f'profit_lag_{i}', lag('profit', offset=i).over(window_lag))\n",
    "\n",
    "# Get lag vars\n",
    "for lag_offset in range(1, 7):\n",
    "    for var_num in range(12, 79):  # feature columns\n",
    "        var = f\"var{var_num}\"\n",
    "        if var in df.columns:\n",
    "            df = df.withColumn(f'var{var_num}_lag{lag_offset}', lag(var, offset=lag_offset).over(window_lag))\n",
    "\n",
    "# drop null values (i.e., the first 10 rows for each trade_id, as they don't have complete lag features)\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop temporary milestone column\n",
    "df = df.drop('milestone')\n",
    "\n",
    "# Convert the date column into timestamp\n",
    "df = df.withColumn('date', to_timestamp('time_stamp', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# Extract year-month from date\n",
    "df = df.withColumn('year_month', date_format('time_stamp', 'yyyy-MM'))\n",
    "\n",
    "# Generate the list of unique year-months\n",
    "year_months = df.select('year_month').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "year_months.sort()\n",
    "\n",
    "# Defining the input columns for the vector assembler\n",
    "input_cols = df.columns\n",
    "input_cols.remove('time_stamp')\n",
    "input_cols.remove('bar_num')\n",
    "input_cols.remove('direction')\n",
    "input_cols.remove('trade_id')\n",
    "input_cols.remove('rank')\n",
    "input_cols.remove('date')\n",
    "input_cols.remove('year_month')\n",
    "input_cols.remove('profit')  # assuming 'profit' is the name of your target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Vector Assembler\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol='features')\n",
    "\n",
    "# Define your model as RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol='profit', featuresCol='features')\n",
    "\n",
    "# Define your evaluator for RMSE and R-Squared\n",
    "evaluator_rmse = RegressionEvaluator(labelCol='profit', metricName=\"rmse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol='profit', metricName=\"r2\")\n",
    "\n",
    "# Init storage results lists\n",
    "r2_values = []\n",
    "mape_values = []\n",
    "output = ''\n",
    "\n",
    "# Open the output file\n",
    "with open('Exercise3.txt', 'w') as f:\n",
    "    for i in range(0, len(year_months)-6, 7):\n",
    "        # Get the train and test data\n",
    "        train_data = df.filter(df.year_month.isin(year_months[i:i+6]))\n",
    "        test_data = df.filter(df.year_month == year_months[i+6])\n",
    "        \n",
    "        # Transform the data\n",
    "        train_data = assembler.transform(train_data)\n",
    "        test_data = assembler.transform(test_data)\n",
    "\n",
    "        # Train your model\n",
    "        rf_model = rf.fit(train_data)\n",
    "        \n",
    "        # Perform inference on the test data\n",
    "        predictions = rf_model.transform(test_data)\n",
    "        \n",
    "        # Evaluate your predictions and store the RMSE and R-Squared result for later analysis\n",
    "        rmse = evaluator_rmse.evaluate(predictions)\n",
    "        r2 = evaluator_r2.evaluate(predictions)\n",
    "        \n",
    "        # Compute MAPE and store it for later analysis\n",
    "        mape = predictions.select(F.avg(F.abs((predictions['profit'] - predictions['prediction']) / predictions['profit']))).alias('mape').collect()[0][0]\n",
    "        \n",
    "        # Write the range of the training timeframe, RMSE, R-squared and MAPE for the current timeframe to file\n",
    "        output += f'Training timeframe: {year_months[i]} to {year_months[i+5]}\\n'\n",
    "        output += '----------------------------------\\n'\n",
    "        output += f'The RMSE for year-month {year_months[i+6]} is {rmse}\\n'\n",
    "        output += f'The R-squared for year-month {year_months[i+6]} is {r2}\\n'\n",
    "        output += f'The MAPE for year-month {year_months[i+6]} is {mape}\\n'\n",
    "        output += f'\\n'\n",
    "    \n",
    "    # Calculate and write overall R-squared, average MAPE, max MAPE and min MAPE to file\n",
    "    overall_r2 = sum(r2_values) / len(r2_values)\n",
    "    avg_mape = sum(mape_values) / len(mape_values)\n",
    "    max_mape = max(mape_values)\n",
    "    min_mape = min(mape_values)\n",
    "    output += f'Overall Results\\n'\n",
    "    output += f'----------------------------------\\n'\n",
    "    output += f'The overall R-squared value: {overall_r2}\\n'\n",
    "    output += f'The average MAPE score: {avg_mape}\\n'\n",
    "    output += f'The maximum MAPE score: {max_mape}\\n'\n",
    "    output += f'The minimum MAPE score: {min_mape}\\n'\n",
    "\n",
    "# Upload the file\n",
    "s3.put_object(Body=output, Bucket=result_bucket, Key=result_location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
